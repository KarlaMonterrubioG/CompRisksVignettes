---
title: "Approaches based on a proportional cause-specific hazard specification"
author: "Karla Monterrubio-Gómez, Nathan Constantine-Cooke, and Catalina Vallejos"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: show
    toc: yes
    css: style.css
    theme: simplex
    highlight: textmate
    toc_float:
      collapsed: no
    includes:
      in_header: head.html
      before_body: navbar.html
    self_contained: false
  pdf_document:
    toc: yes
link-citations: yes
bibliography: references.bib
vignette: "%\\VignetteIndexEntry{Vignette Title} %\\VignetteEngine{knitr::rmarkdown}
  %\\VignetteEncoding{UTF-8}\n"
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  cache = FALSE
)
```

# Introduction

The dataset used here corresponds to the Hodgkin’s disease (HD) study described
in Pintilie, 2006. The data was loaded and pre-processed using the script that
is sourced below. The latter creates three data objects:

- `hd`: all the samples in the original data
- `hd_train`: a random sample of the data to be used as a training set
- `hd_test`: a random sample of the data to be used as a testing set

Details about data pre-processing steps can be found [here](https://github.com/VallejosGroup/CompRisksVignettes/blob/main/Source/DataPreparation.Rmd)

```{r}
source("../Data_prep/data_prep.R")
```

The following sections illustrate the usage of different methods defined using 
a cause-specific hazards formulation. 

# Cause-specific Cox Proportional-Hazards

There are several R packages that one can employ to fit a cause-specific
hazard regression model. For this purpose, separate Cox proportional hazards
models are fitted for each event type, treating other events as censored 
observations. In this section, we explore how we can perform this
task with different R packages. We discuss its similarities and differences:
highlighting important aspects to consider in each case.

## The `survival` package

The model can be fit with the `coxph()` function in the `survival` package. In
order to do so, one should first create a survival time object with the `Surv()`
function. This object will be the response variable in our regression model.
For illustration purposes, we only fit the cause-specific model associated to
events of type 1. To do this, in the code below, we have set `status==1` in the 
second argument of `Surv()` to indicate that we are interested in event type 1. 
This setting will treat the other status values as censored observations. The 
cause-specific model for the second event type can be fitted using `status==2`.

```{r, message = FALSE}
require(survival)
csh_survival <- coxph(Surv(time, status == 1) ~ age +
                        sex +
                        trtgiven +
                        medwidsi +
                        extranod +
                        clinstg,
                       data = hd_train,
                       x = TRUE,         # needed for predictions
                       # this is the default, one can also use Breslow
                       ties = "efron")
summary(csh_survival)
```

The above output summarises the fitted model. First, it shows that out of the
`r nrow(hd_train)` patients, `r sum(hd_train$status == 1)` experienced the event 
of interest (relapse). Second, it shows the coefficients and hazard ratios 
(`exp(coef)`) for each covariate, along with the results of a marginal 
significance test. The results suggest that age, treatment, and clinical stage 
are significantly associated with the hazard of relapse. For instance, being in 
clinical stage 2 increases the cause-specific hazard (rate) of relapse by 
`r round((exp(coef(csh_survival)[7]) - 1)*100,2)`% when compared to a patient in 
stage 1. It is important to emphasise that one should be cautious when 
interpreting these covariate effects as this modelling strategy only permits to 
do inference of the effects on the hazard but not on the prognosis or survival 
(see more details in @Austin2016). Naively estimating survival/risk probabilities 
from this model, will result in overestimation of the survival function, as 
competing risks are not taken into account. See the [riskRegression section](#the-riskregression-package) for a valid approach.
Finally, the output also reports model fitness statistics, including the 
concordance index, which summaries the in-sample discriminative ability of the 
model (a value close to 1 is preferred).b Note, however, that none of these 
metrics account for competing events. 
 
## The `rms` package

Another package that permits to fit the model is `rms`. This package provides
useful functions for model validation, and plotting which are well documented in
@harrell2017.

Before fitting the model, it is important to compute summary statistics of the
covariates that will be employed. Such statistics will be used when plotting or
doing predictions. These summary statistics are computed using `datadist()`
function.

As in the previous section, we only fit the cause specific model for the first
event type. The code for the model associated to the second event type is 
analogous, using `status == 2` in the `Surv()` call. 

```{r, message = FALSE}
require(rms)

units(hd$time) <- "years"
dd <- datadist(hd)
options(datadistc = "dd")

csh_rms <- cph(Surv(time, status == 1) ~ age +
                                        sex +
                                        trtgiven +
                                        medwidsi +
                                        extranod +
                                        clinstg,
                data = hd_train,
                x = TRUE,
                y = TRUE,
                surv = TRUE,
                method = "efron")  # default is efron, one can also use breslow

print(csh_rms)
```

In this case, printing the fitted model, gives a slightly different output. 
First, the output includes several discrimination indices. Note that, 
the concordance index can be recovered using the `Dxy` index. Moreover, note 
that these metrics ignore the presence of competing event types. 

`summary.rms()` is of more utility to compute hazard ratios. For continuous
covariates, the HR are computed with respect to the lower and upper quartiles.
For instance, from the output below we can infer that a 20-year increase in age
(going from 23 to 43), increases the cause-specific hazard of relapse by 37.38%  .
This is in contrast to the coefficients obtained by `print()`, which in this
case corresponds to `exp(coef)`=`r round(exp(coef(csh_rms)[1]),2)` and whose 
interpretation refers to the effect on the hazard by 1-year increase. 

```{r}
summary(csh_rms)
```

Furthermore, one can manually define ranges in which we want to compute HR; for
instance going from 30-40 years, i.e. a 10-year increase will result in a 
increase in the cause-specific hazard of 17.21%, as shown below. Similarly, for 
discrete covariates, one can define the reference level with respect to which 
the HR are computed; for example, below we have changed the reference level for 
sex.

```{r}
summary(csh_rms, age = c(30, 40), sex = "F")
```

Note that naively estimating survival probabilities from this model, will result
in overestimation of the survival function, as competing risks are not taken
into account. See the next section for a valid approach.

## The `riskRegression` package

One can also use the `CSC()` function from `riskRegression` to fit
cause-specific hazard regression models. This package acts as an interface to
obtain either a `coxph` or a `cph` object through the `fitter` argument.
In addition, this function simultaneously fits models for both competing events
when `surv.type = "hazard"`. Below, we employ the same covariates for
both event types, but it is possible to fit different regression models for each 
cause (see example below). Note that in this case, the response variable in the 
regression model is obtained with the `Hist()` function available in the 
`prodlim` R package.

```{r, message = FALSE}
require(riskRegression)
require(prodlim) # for Hist()
csh_riskRegression <- CSC(Hist(time, status) ~ age +
                                               sex +
                                               trtgiven +
                                               medwidsi +
                                               extranod +
                                               clinstg,
                          data = hd_train,
                          surv.type = "hazard",
                          cause = 1)
print(csh_riskRegression)
```

Now the output shows model summaries for both causes. If we analyse Cause: 1,
the results should be identical to those obtained with `coxph()` in the survival
package, and are interpreted in the same way. 

Results obtained by using a `cph` fitter are the same as above and are shown
below:

```{r}
csh_riskRegression.cph <- CSC(Hist(time, status) ~ age +
                                                   sex +
                                                   trtgiven +
                                                   medwidsi +
                                                   extranod + 
                                                   clinstg,
                              data = hd_train,
                              surv.type = "hazard",
                              fitter = "cph")
print(csh_riskRegression.cph)
```

Assume that we are interested in making risk predictions at time $t=5$ years for 
a new set of patients (i.e. the test dataset `hd_test`). As an illustration, we 
focus on predicting whether an event of type 1 will occur by that time (to 
predict the occurrence of the second event type, use `cause = 2` in the code 
below). The `riskRegression` package permits obtaining absolute risk predictions 
at specific time points. For each patient in this new set, this consists of 
calculating the probability of observing an event of type 1 by 5 years. Here, 
we set `product.limit = FALSE`, to use the exponential approximation
$S(t)=exp(-H(t)_1 - H(t)_2)$, where $H(t)_j$ denotes the cumulative hazard for
cause $j$ at time $t$.

Below, we show the predicted risk for the first 5 patients in the test dataset.
We see how both fitters (`coxph` or `cph`) give the same result:
```{r}
risk.coxph <- predictRisk(csh_riskRegression,
                          times = 5,
                          newdata = hd_test, cause = 1,
                          product.limit = FALSE)
risk.coxph[1:5]


risk.cph <- predictRisk(csh_riskRegression.cph,
                        times = 5,
                        newdata = hd_test,
                        cause = 1,
                        product.limit = FALSE)
risk.cph[1:5]
```

If instead we use the product limit estimator, we ensure
$\text{CIF}_1(t | X) + \text{CIF}_2(t|X) = 1$ and the output is
slightly different:

```{r}
risk.coxphPL <- predictRisk(csh_riskRegression,
                            times = 5,
                            newdata = hd_test,
                            cause = 1,
                            product.limit = TRUE)
risk.coxphPL[1:5]

risk.cphPL  <- predictRisk(csh_riskRegression.cph,
                           times = 5,
                           newdata = hd_test,
                           cause = 1,
                           product.limit = TRUE)
risk.coxphPL[1:5]
```

Note that the product limit estimator is also employed when using
`predictEventProb()` available in `pec` package. 

```{r}
require(pec)
risk.pec <- predictEventProb(csh_riskRegression.cph,
                             newdata = hd_test,
                             cause = 1,
                             time = 5)
risk.pec[1:5]
```

Finally, note also that `riskRegression` permits to fit different regression
models for each cause using Cox proportional-hazards. In the example below, 
we use all covariates for cause 1, and we employ only age, and sex for cause 2.

```{r, message = FALSE}
# Different regression models for each cause.
csh_riskRegression2 <- CSC(list(Hist(time, status) ~ age + sex + trtgiven +
                                            medwidsi + extranod + clinstg,
                                Hist(time, status) ~ age + sex),
                           data = hd_train)
print(csh_riskRegression2)
```

# Sparse regression

## Lasso - The `glmnet` package

The `glmnet()` function permits to fit different Cox sparse regression models
through the argument `family="cox"`. It is possible to use lasso, ridge and
elastic net regularization. Here, we used lasso.

As in previous examples, the code below fits a cause-specific proportional 
hazards model for the first event type only.

First, as `glmnet` cannot directly handle factor variables, we extract the
design matrix used in previous models (with re-codes factor variables into
dummy binary variables). Second, the response variable is defined using a 
`Surv` object. Finally, the argument `alpha = 1` indicates that a lasso penalty 
will be used. Note that, users can set `alpha = 0` to select a ridge penalty. 
If the value of `alpha` lies between 0 and 1, an elastic net penalty will be 
used instead. 

```{r}
require(glmnet)

x <- model.matrix(csh_survival) 

lasso.fit <- glmnet(x = x,
                    y = Surv(hd_train$time, hd_train$status == 1),
                    family = "cox",
                    # alpha=0 for ridge and  0<alpha<1 for elastic net
                    alpha = 1)
```

The output of the fitted model shows degrees of freedom and deviance for different
values of $\lambda$.

```{r}
print(lasso.fit)
```

In the plot below, each line corresponds to one of the covariates. The top
x-axis indicates the number of non-zero coefficients for different values of
$\lambda$ (bottom x-axis).

```{r, fig.width = 8, fig.height = 6}
plot(lasso.fit, label = TRUE)
```

The estimated coefficients for specific $\lambda$ values can be obtained using
`coef()`. For example, for $\lambda = 0.06$:

```{r}
coef(lasso.fit, s = 0.06) # s:=\lambda
```

Note that the interpretation of the regression coefficients is similar to those 
presented in earlier sections.  

One can also employ k-fold cross-validation to select the optimal value of
$\lambda$. Below, we show how to do that using 10 folds and Harrel's concordance
index (one can also employ a deviance loss measure). Note, however, that none of 
these metrics account for competing events. 

```{r}
set.seed(1)
lassoCV.fit <- cv.glmnet(x = x,
                         y = Surv(hd_train$time, hd_train$status == 1),
                         family = "cox",
                         alpha = 1,              # this determines lasso fit
                         nfolds = 10,
                         type.measure = "C")     # uses Harrel's C-index
```


```{r, fig.width = 6, fig.height = 4}
plot(lassoCV.fit)
```

The specific $\lambda$ values marked with the left and right vertical lines in
the plot shown above, can be obtained with the following commands.

```{r}
lassoCV.fit$lambda.min; lassoCV.fit$lambda.1se
```

The first value (`lambda.min`) corresponds to the $\lambda$ value for which the 
cross-validation performance is best, i.e. it leads to the highest C-index in 
this example. In principle, `lambda.min` can be used as an optimal choice. In
such case, the final model includes all covariates (only the coefficient 
associated to `medwidsiN` is shrunk to be exactly equal to zero):

```{r}
coef(lassoCV.fit, s = lassoCV.fit$lambda.min)
```

Note that, for these data, several values of $\lambda$ (leading to models with 
varying levels of sparsity) result in similar cross-validated performance. 
Hence, choosing `lambda.min` may not be appropriate. An alternative, more
parsimonious, choice can be selected by using `lambda.1se` [@hastie2009elements]. 
In such case, `lambda.1se` represents the highest value of $\lambda$ (i.e. the 
strongest regularisation) such that the chosen performance metric (C-index here) 
is within 1 standard deviation of the one associated to `lambda.min`. In this 
example, the selected model only contains 3 covariates (`age`, `trtgiven` and
`medwidsi`):

```{r}
coef(lassoCV.fit, s = lassoCV.fit$lambda.1se)
```

## Cox model-based boosting - The `mboost` package
 
A cause-specific CPH model that employs model-based boosting is showcased here
by using the `glmboost()` function. As with other approaches, one should first
create the response variable using `Surv()`. 

Because `mboost` is an R package that can fit models beyond survival analysis, 
one should specify `family = CoxPH()`. The parameters of the boosting algorithm 
are then set by the `control` argument. In the example below we have used 1000 
boosting iterations with the step-length (referred as $\lambda$ in the 
manuscript; `nu` in the code below) set at 0.3. In addition, we indicate that 
continuous variables have not been previously centered. 

As before, for illustration purposes, we only fit the model associated to the 
first event type.

```{r mboost, message = FALSE}
require(mboost)

csh_mboost <- glmboost(Surv(time, status == 1) ~ age +
                         sex +
                         trtgiven +
                         medwidsi +
                         extranod +
                         clinstg,
                       data = hd_train,
                       family = CoxPH(),
                       control = boost_control(mstop = 1000,
                                               nu = 0.3),
                       center = FALSE)

print(csh_mboost)
```

A plot of the coefficients across boosting iterations is shown:

```{r, fig.width = 8, fig.height = 6}
plot(csh_mboost, main = "Coefficient paths", ylim = range(coef(csh_mboost)))
```

To do variable selection one can use the `varimp()` function. In the plot below
we observe that age, treatment given, clinical stage and size of mediastinum
involvement appear to be the most relevant covariates.

```{r, fig.width = 6, fig.height = 4}
plot(varimp(csh_mboost))
```

`mboost` permits cross-validation for hyper-parameter selection; specifically, 
the number of boosting iterations. Here, we show how to do this using 25 
bootstrap samples (default). Note that the value of the second hyper-parameter, 
`nu`, is not critical as long as it is sufficiently large. As such 
cross-validation for `nu` is not generally recommended.  

```{r,  echo=T, results='hide'}
set.seed(1)
cv.boost <- cvrisk(csh_mboost, papply = lapply)
```

The next plot shows how the (negative) cross-validated likelihood (cvl) 
[@verweij1993cross] changes as the number of boosting iterations increases. 
Light grey lines represent each of the 25 boostrap samples. The black line shows 
average values across all boostrap samples.

```{r}
plot(cv.boost)
```

The function `mstop` can be used to identify the optimal number of boosting
iteration. As a default, this is chosen to minimize the negative cvl. 

```{r}
stop.B <- mstop(cv.boost)  # can serve to obtain optimal stopping parameter
stop.B 
```

To better visualise the behavior of cvl, we also plot the black line alone 
(i.e. the mean of cvl across boostrap samples):

```{r}
mean.cvl <- colMeans(cv.boost)
plot(mean.cvl, ylab = "cvl", xlab = "Number of boosting iterations", type = "l")
abline(v = stop.B, col = "red")
```

In some cases, cvl may be a relatively flat function of the number of boosting 
iterations (within certain range). In those cases, similar to the use of 
`lambda.1se` in penalised regression (see previous section), it may 
also be possible to select a more parsimonious model with similar 
cross-validated error. For example, 250 boosting iterations lead to 
similar (to 3 decimal points) as 292 iterations (`stop.B`).

```{r}
mean.cvl[250+1]
mean.cvl[stop.B+1]
```

Once the number of boosting iterations has been selected, the associated 
regression coefficients can be directly obtained from the model object.

```{r}
coef(csh_mboost[250])
coef(csh_mboost[stop.B])
```

Note, if the plots above suggest that cvl has not yet achieved stable values,
a larger number of boosting iterations may be required. The latter can be 
obtained through the fitted model, without the need to re-run the initial 
iterations. For example, the following code returns a model with 1200 boosting
iterations:

```{r}
csh_mboost[1200]
```

# Comparison: estimates of regression coefficients

First, we compare estimated regression coefficients obtained above. As the 
estimates obtained for the cause-specific Cox PH were the same regardless of the 
choice of R library (`survival`, `rms` o `riskRegression`), we only store one
of them (the ones for `survival`)

```{r}
# Adds zeros for the results of mboost as only non-zero coefficients are returned
coef_boost_250 <- coef(csh_mboost[250])
coef_boost_250 <- c(coef_boost_250[1], 0, coef_boost_250[2:6])
df <- data.frame("cox" = summary(csh_survival)$coefficients[,1],
                 "cox_se" = summary(csh_survival)$coefficients[,3],
                 "lasso_min" = coef(lassoCV.fit, s = lassoCV.fit$lambda.min)[,1],
                 "lasso_1se" = coef(lassoCV.fit, s = lassoCV.fit$lambda.1se)[,1],
                 "mboost_stopb" = coef(csh_mboost[stop.B]),
                 "mboost_250" = coef_boost_250)
round(df, 2)
df$variable <- rownames(df)
```


The following code was used to create Figure 1A:

```{r}
# colour version
library(ggplot2)
library(patchwork)

p_all <- ggplot(df, aes(x=variable, y=cox, col = "Cox")) + 
  geom_errorbar(aes(ymin = cox - cox_se, ymax = cox + cox_se), width=.1) +
  geom_point(cex = 2.5) + 
  geom_point(aes(x = variable, y = lasso_min, col = "lasso_min"), cex = 2.5) +
  geom_point(aes(x = variable, y = lasso_1se, colour = "lasso_1se"), cex = 2.5) +
  geom_point(aes(x = variable, y = mboost_stopb, colour = "mboost_min"), cex = 2.5) + 
  geom_point(aes(x = variable, y = mboost_250, colour = "mboost_other"), cex = 2.5) +
  geom_hline(yintercept = 0, lty = 2, colour = "gray") +
  theme_classic() +
  ylab("Estimate") + xlab("Coefficient") +
  labs(color="Method") +
  scale_color_manual(values=c("#999999", "#1A5276", "#2980B9", "#B9770E", "#F5B041"))

p_age <- ggplot(df[df$variable == "age",], aes(x=variable, y=cox, col = "Cox")) + 
  geom_errorbar(aes(ymin = cox - cox_se, ymax = cox + cox_se), width=.1) +
  geom_point(cex = 2.5) + 
  geom_point(aes(x = variable, y = lasso_min, col = "lasso_min"), cex = 2.5) +
  geom_point(aes(x = variable, y = lasso_1se, colour = "lasso_1se"), cex = 2.5) +
  geom_point(aes(x = variable, y = mboost_stopb, colour = "mboost_min"), cex = 2.5) + 
  geom_point(aes(x = variable, y = mboost_250, colour = "mboost_other"), cex = 2.5) +
  theme_classic() +
  ylab("Estimate") + xlab("Coefficient") +
  labs(color="Method") +
  scale_color_manual(values=c("#999999", "#1A5276", "#2980B9", "#B9770E", "#F5B041")) +
  theme(legend.position="none", 
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        plot.background = element_rect(colour = "black", fill=NA, size=0.6))

p_all + inset_element(p_age, left = 0.01, bottom = 0.6, right = 0.2, top = 1)

if (file.exists("/.dockerenv")){ # running in docker
  ggsave("/Outputs/Comparison_estimation_CSH.pdf", device = "pdf")
} else {
  ggsave("../Outputs/Comparison_estimation_CSH.pdf", device = "pdf")
}
```

```{r}
# black and white

p_all <- ggplot(df, aes(x=variable, y=cox, pch = "Cox")) + 
  geom_errorbar(aes(ymin = cox - cox_se, ymax = cox + cox_se), width=.1) +
  geom_point(cex = 2.5) + 
  geom_point(aes(x = variable, y = lasso_min, pch = "lasso_min"), cex = 2.5) +
  geom_point(aes(x = variable, y = lasso_1se, pch =  "lasso_1se"), cex = 2.5) +
  geom_point(aes(x = variable, y = mboost_stopb, pch =  "mboost_min"), cex = 2.5) + 
  geom_point(aes(x = variable, y = mboost_250, pch = "mboost_other"), cex = 2.5) +
  geom_hline(yintercept = 0, lty = 2, colour = "gray") +
  theme_classic() +
  ylab("Estimate") + xlab("Coefficient") +
  labs(pch="Method") +
  scale_shape_manual(values = c(16, 15, 17, 1, 2)) 

p_age <- ggplot(df[df$variable == "age",], aes(x=variable, y=cox, pch = "Cox")) + 
  geom_errorbar(aes(ymin = cox - cox_se, ymax = cox + cox_se), width=.1) +
  geom_point(cex = 2.5) + 
  geom_point(aes(x = variable, y = lasso_min, pch = "lasso_min"), cex = 2.5) +
  geom_point(aes(x = variable, y = lasso_1se, pch = "lasso_1se"), cex = 2.5) +
  geom_point(aes(x = variable, y = mboost_stopb, pch = "mboost_min"), cex = 2.5) + 
  geom_point(aes(x = variable, y = mboost_250, pch = "mboost_other"), cex = 2.5) +
  theme_classic() +
  ylab("Estimate") + xlab("Coefficient") +
  labs(pch="Method") +
  theme(legend.position="none", 
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        plot.background = element_rect(colour = "black", fill=NA, size=0.6)) +
  scale_shape_manual(values = c(16, 15, 17, 1, 2)) 

p_all + inset_element(p_age, left = 0.01, bottom = 0.6, right = 0.2, top = 1)

if (file.exists("/.dockerenv")){ # running in docker
  ggsave("/Outputs/Comparison_estimation_CSH_BW.pdf", device = "pdf")
} else {
  ggsave("../Outputs/Comparison_estimation_CSH_BW.pdf", device = "pdf")
}
```

# Storing predictions

In order to allow comparison with the predictions generated by other methods,
we save the predictions obtained in this vignette.  

```{r save_pred}
pred_CS <- data.frame("testID" = seq_len(nrow(hd_test)),
                      "coxph_riskRegression" = risk.coxph,
                      "coxphPL_riskRegression" = risk.pec)
if (file.exists("/.dockerenv")){ # running in docker
  write.csv(pred_CS, "/Outputs/pred_CS.csv", row.names = FALSE)
} else {
  write.csv(pred_CS, "../Outputs/pred_CS.csv", row.names = FALSE)
}
```

# References
<div id="refs"></div>

# Session Info

```{r}
sessionInfo()
```
